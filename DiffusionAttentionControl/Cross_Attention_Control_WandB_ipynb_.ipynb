{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDLvF9KJf_mV"
   },
   "source": [
    "# üî•üî• Cross-Attention Control with Stable Diffusion + WandB Playground ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{cross-attention-control-tmp} -->\n",
    "\n",
    "An implementation of Prompt-to-Prompt Image Editing\n",
    "with Cross Attention Control using [Stable Diffusion](https://github.com/CompVis/stable-diffusion), [HuggingFace Diffusers](https://github.com/huggingface/diffusers) and [Weights & Biases](https://wandb.ai/site)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J1IpII-kK0-"
   },
   "source": [
    "# Step 1: Setup required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_rIUU9kd2cV",
    "outputId": "1d1aa835-29a9-4bcb-ddac-9b25d1dd276d"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "!pip install -q diffusers transformers ftfy wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ty2rvG3rePwe"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "import io\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import torch\n",
    "from torch import autocast\n",
    "\n",
    "from transformers import CLIPModel, CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import (\n",
    "    AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55695b18f1fb4fd2a55cf94b9b883af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icR8uhL9kUI2"
   },
   "source": [
    "# Step 2: Set up Models and Weights & Biases Run\n",
    "\n",
    "- `wandb_project`: Weights & Biases project.\n",
    "- `wandb_project`: Weights & Biases entity.\n",
    "- `huggingface_access_token`: HuggingFace Access Token. Check out this page from the official HuggingFace docs as to how to generate your own access token.\n",
    "- `config.device`: Accelerator device. Choose `mps` if you're running the code on an M1 Mac.\n",
    "- `config.model_path_clip`: Alias for pre-trained CLIP Model.\n",
    "- `config.model_path_diffusion`: Alias for pre-trained Stable Diffusion Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "id": "mk2Mogzwecpo",
    "outputId": "45ff2b82-6a06-4794-be62-461e023131d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uy9cjbia) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">clear-snow-2</strong>: <a href=\"https://wandb.ai/simmax21/CrossAttentionControl/runs/uy9cjbia\" target=\"_blank\">https://wandb.ai/simmax21/CrossAttentionControl/runs/uy9cjbia</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221020_190506-uy9cjbia/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uy9cjbia). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e58d5cba474123a7111c2e4c7449a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.033336758613586426, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/m_bobrin/Text2imgScratch/DiffusionAttentionControl/wandb/run-20221020_190701-1tnup80t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simmax21/CrossAttentionControl/runs/1tnup80t\" target=\"_blank\">vivid-wave-2</a></strong> to <a href=\"https://wandb.ai/simmax21/CrossAttentionControl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b14c08f84f490184d1f3e2b2eff427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf0f8eec629476281005d7323c2c6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd46d4f1c3c5484d911bae13e350bb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabbf858e6064c6389a14384c4ff8705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/550 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextTransformer(\n",
       "  (embeddings): CLIPTextEmbeddings(\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "    (position_embedding): Embedding(77, 768)\n",
       "  )\n",
       "  (encoder): CLIPEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_project = \"CrossAttentionControl\" #@param {\"type\": \"string\"}\n",
    "wandb_entity = \"\" #@param {\"type\": \"string\"}\n",
    "\n",
    "wandb.init(project=wandb_project, job_type=\"generate\")\n",
    "config = wandb.config\n",
    "\n",
    "huggingface_access_token = \"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\" #@param {\"type\": \"string\"}\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "config.model_precision_type = \"fp16\"\n",
    "config.device = \"cuda\" #@param['cuda', 'cpu', 'mps']\n",
    "config.model_path_clip = \"openai/clip-vit-large-patch14\" #@param['openai/clip-vit-large-patch14']\n",
    "config.model_path_diffusion = \"CompVis/stable-diffusion-v1-4\" #@param['CompVis/stable-diffusion-v1-4']\n",
    "\n",
    "\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(config.model_path_clip)\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "    config.model_path_clip,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "clip = clip_model.text_model\n",
    "\n",
    "\n",
    "model_path_diffusion = \"CompVis/stable-diffusion-v1-4\"\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_path_diffusion,\n",
    "    subfolder=\"unet\",\n",
    "    use_auth_token=huggingface_access_token,\n",
    "    revision=config.model_precision_type,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    model_path_diffusion,\n",
    "    subfolder=\"vae\",\n",
    "    use_auth_token=huggingface_access_token,\n",
    "    revision=config.model_precision_type,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "unet.to(config.device)\n",
    "vae.to(config.device)\n",
    "clip.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6iZXdCVPfLDc"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "def init_attention_weights(weight_tuples):\n",
    "    tokens_length = clip_tokenizer.model_max_length\n",
    "    weights = torch.ones(tokens_length)\n",
    "    \n",
    "    for i, w in weight_tuples:\n",
    "        if i < tokens_length and i >= 0:\n",
    "            weights[i] = w\n",
    "    \n",
    "    \n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.last_attn_slice_weights = weights.to(config.device)\n",
    "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
    "            module.last_attn_slice_weights = None\n",
    "    \n",
    "\n",
    "def init_attention_edit(tokens, tokens_edit):\n",
    "    tokens_length = clip_tokenizer.model_max_length\n",
    "    mask = torch.zeros(tokens_length)\n",
    "    indices_target = torch.arange(tokens_length, dtype=torch.long)\n",
    "    indices = torch.zeros(tokens_length, dtype=torch.long)\n",
    "\n",
    "    tokens = tokens.input_ids.numpy()[0]\n",
    "    tokens_edit = tokens_edit.input_ids.numpy()[0]\n",
    "    \n",
    "    for name, a0, a1, b0, b1 in SequenceMatcher(\n",
    "        None, tokens, tokens_edit\n",
    "    ).get_opcodes():\n",
    "        if b0 < tokens_length:\n",
    "            if name == \"equal\" or (name == \"replace\" and a1-a0 == b1-b0):\n",
    "                mask[b0:b1] = 1\n",
    "                indices[b0:b1] = indices_target[a0:a1]\n",
    "\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.last_attn_slice_mask = mask.to(config.device)\n",
    "            module.last_attn_slice_indices = indices.to(config.device)\n",
    "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
    "            module.last_attn_slice_mask = None\n",
    "            module.last_attn_slice_indices = None\n",
    "\n",
    "\n",
    "def init_attention_func():\n",
    "    def new_attention(self, query, key, value, sequence_length, dim):\n",
    "        batch_size_attention = query.shape[0]\n",
    "        hidden_states = torch.zeros(\n",
    "            (batch_size_attention, sequence_length, dim // self.heads), device=query.device, dtype=query.dtype\n",
    "        )\n",
    "        slice_size = self._slice_size if self._slice_size is not None else hidden_states.shape[0]\n",
    "        for i in range(hidden_states.shape[0] // slice_size):\n",
    "            start_idx = i * slice_size\n",
    "            end_idx = (i + 1) * slice_size\n",
    "            attn_slice = (\n",
    "                torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale\n",
    "            )\n",
    "            attn_slice = attn_slice.softmax(dim=-1)\n",
    "            \n",
    "            if self.use_last_attn_slice:\n",
    "                if self.last_attn_slice_mask is not None:\n",
    "                    new_attn_slice = torch.index_select(self.last_attn_slice, -1, self.last_attn_slice_indices)\n",
    "                    attn_slice = attn_slice * (1 - self.last_attn_slice_mask) + new_attn_slice * self.last_attn_slice_mask\n",
    "                else:\n",
    "                    attn_slice = self.last_attn_slice\n",
    "                \n",
    "                self.use_last_attn_slice = False\n",
    "                    \n",
    "            if self.save_last_attn_slice:\n",
    "                self.last_attn_slice = attn_slice\n",
    "                self.save_last_attn_slice = False\n",
    "                \n",
    "            if self.use_last_attn_weights and self.last_attn_slice_weights is not None:\n",
    "                attn_slice = attn_slice * self.last_attn_slice_weights\n",
    "                self.use_last_attn_weights = False\n",
    "\n",
    "            attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx])\n",
    "\n",
    "            hidden_states[start_idx:end_idx] = attn_slice\n",
    "\n",
    "        # reshape hidden_states\n",
    "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\":\n",
    "            module.last_attn_slice = None\n",
    "            module.use_last_attn_slice = False\n",
    "            module.use_last_attn_weights = False\n",
    "            module.save_last_attn_slice = False\n",
    "            module._attention = new_attention.__get__(module, type(module))\n",
    "            \n",
    "def use_last_tokens_attention(use=True):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.use_last_attn_slice = use\n",
    "            \n",
    "def use_last_tokens_attention_weights(use=True):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.use_last_attn_weights = use\n",
    "            \n",
    "def use_last_self_attention(use=True):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
    "            module.use_last_attn_slice = use\n",
    "            \n",
    "def save_last_tokens_attention(save=True):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
    "            module.save_last_attn_slice = save\n",
    "            \n",
    "def save_last_self_attention(save=True):\n",
    "    for name, module in unet.named_modules():\n",
    "        module_name = type(module).__name__\n",
    "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
    "            module.save_last_attn_slice = save\n",
    "\n",
    "\n",
    "def postprocess(image):\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    image = (image[0] * 255).round().astype(\"uint8\")\n",
    "    return Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "0_TrNFOhqQxx"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "@torch.no_grad()\n",
    "def stablediffusion(\n",
    "    prompt=\"\",\n",
    "    prompt_edit=\"\",\n",
    "    prompt_edit_token_weights=[],\n",
    "    prompt_edit_tokens_start=0.0,\n",
    "    prompt_edit_tokens_end=1.0,\n",
    "    prompt_edit_spatial_start=0.0,\n",
    "    prompt_edit_spatial_end=1.0,\n",
    "    guidance_scale=7.5,\n",
    "    steps=50,\n",
    "    seed=None,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    init_image=None,\n",
    "    init_image_strength=0.5,\n",
    "):\n",
    "    log_key = (\n",
    "        \"Generated Image without Promp Edit\"\n",
    "        if prompt_edit == \"\"\n",
    "        else \"Generated Image with Promp Edit\"\n",
    "    )\n",
    "    print(log_key)\n",
    "\n",
    "    # Change size to multiple of 64 to prevent size mismatches inside model\n",
    "    width = width - width % 64\n",
    "    height = height - height % 64\n",
    "    \n",
    "    #If seed is None, randomly select seed from 0 to 2^32-1\n",
    "    if seed is None: seed = random.randrange(2**32 - 1)\n",
    "    generator = torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Set inference timesteps to scheduler\n",
    "    scheduler = LMSDiscreteScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000\n",
    "    )\n",
    "    scheduler.set_timesteps(steps)\n",
    "    \n",
    "    # Preprocess image if it exists (img2img)\n",
    "    if init_image is not None:\n",
    "        #Resize and transpose for numpy b h w c -> torch b c h w\n",
    "        init_image = init_image.resize(\n",
    "            (width, height), resample=Image.LANCZOS\n",
    "        )\n",
    "        init_image = np.array(\n",
    "            init_image\n",
    "        ).astype(np.float32) / 255.0 * 2.0 - 1.0\n",
    "        init_image = torch.from_numpy(\n",
    "            init_image[np.newaxis, ...].transpose(0, 3, 1, 2)\n",
    "        )\n",
    "        \n",
    "        # If there is alpha channel, composite alpha for white,\n",
    "        # as the diffusion model does not support alpha channel\n",
    "        if init_image.shape[1] > 3:\n",
    "            init_image = init_image[:, :3] * init_image[:, 3:] + (\n",
    "                1 - init_image[:, 3:]\n",
    "            )\n",
    "            \n",
    "        #Move image to GPU\n",
    "        init_image = init_image.to(config.device)\n",
    "        \n",
    "        #Encode image\n",
    "        with autocast(config.device):\n",
    "            init_latent = vae.encode(\n",
    "                init_image\n",
    "            ).latent_dist.sample(generator=generator) * 0.18215\n",
    "            \n",
    "        t_start = steps - int(steps * init_image_strength)\n",
    "            \n",
    "    else:\n",
    "        init_latent = torch.zeros(\n",
    "            (1, unet.in_channels, height // 8, width // 8),\n",
    "            device=config.device\n",
    "        )\n",
    "        t_start = 0\n",
    "    \n",
    "    # Generate random normal noise\n",
    "    noise = torch.randn(\n",
    "        init_latent.shape, generator=generator, device=config.device\n",
    "    )\n",
    "    latent = scheduler.add_noise(\n",
    "        init_latent, noise, t_start\n",
    "    ).to(config.device)\n",
    "    \n",
    "    # Process clip\n",
    "    with autocast(config.device):\n",
    "        tokens_unconditional = clip_tokenizer(\n",
    "            \"\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=clip_tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_overflowing_tokens=True\n",
    "        )\n",
    "        embedding_unconditional = clip(\n",
    "            tokens_unconditional.input_ids.to(config.device)\n",
    "        ).last_hidden_state\n",
    "\n",
    "        tokens_conditional = clip_tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=clip_tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_overflowing_tokens=True\n",
    "        )\n",
    "        embedding_conditional = clip(\n",
    "            tokens_conditional.input_ids.to(config.device)\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # Process prompt editing\n",
    "        if prompt_edit != \"\":\n",
    "            tokens_conditional_edit = clip_tokenizer(\n",
    "                prompt_edit,\n",
    "                padding=\"max_length\",\n",
    "                max_length=clip_tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_overflowing_tokens=True\n",
    "            )\n",
    "            embedding_conditional_edit = clip(\n",
    "                tokens_conditional_edit.input_ids.to(config.device)\n",
    "            ).last_hidden_state\n",
    "            \n",
    "            init_attention_edit(\n",
    "                tokens_conditional, tokens_conditional_edit\n",
    "            )\n",
    "            \n",
    "        init_attention_func()\n",
    "        init_attention_weights(prompt_edit_token_weights)\n",
    "            \n",
    "        timesteps = scheduler.timesteps[t_start:]\n",
    "        \n",
    "        for i, t in tqdm(enumerate(timesteps), total=len(timesteps)):\n",
    "            t_index = t_start + i\n",
    "\n",
    "            sigma = scheduler.sigmas[t_index]\n",
    "            latent_model_input = latent\n",
    "            latent_model_input = (\n",
    "                latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "            ).to(unet.dtype)\n",
    "\n",
    "            # Predict the unconditional noise residual\n",
    "            noise_pred_uncond = unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=embedding_unconditional\n",
    "            ).sample\n",
    "            \n",
    "            # Prepare the Cross-Attention layers\n",
    "            if prompt_edit is not None:\n",
    "                save_last_tokens_attention()\n",
    "                save_last_self_attention()\n",
    "            else:\n",
    "                #Use weights on non-edited prompt when edit is None\n",
    "                use_last_tokens_attention_weights()\n",
    "                \n",
    "            # Predict the conditional noise residual and save\n",
    "            # the cross-attention layer activations\n",
    "            noise_pred_cond = unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=embedding_conditional\n",
    "            ).sample\n",
    "            \n",
    "            # Edit the Cross-Attention layer activations\n",
    "            if prompt_edit != \"\":\n",
    "                t_scale = t / scheduler.num_train_timesteps\n",
    "                if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end:\n",
    "                    use_last_tokens_attention()\n",
    "                if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end:\n",
    "                    use_last_self_attention()\n",
    "                    \n",
    "                # Use weights on edited prompt\n",
    "                use_last_tokens_attention_weights()\n",
    "\n",
    "                # Predict the edited conditional noise residual\n",
    "                # using the cross-attention masks\n",
    "                noise_pred_cond = unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=embedding_conditional_edit\n",
    "                ).sample\n",
    "                \n",
    "            #Perform guidance\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "                noise_pred_cond - noise_pred_uncond\n",
    "            )\n",
    "\n",
    "            latent = scheduler.step(noise_pred, t_index, latent).prev_sample\n",
    "\n",
    "            wandb.log({\n",
    "                log_key: wandb.Image(\n",
    "                    postprocess(\n",
    "                        vae.decode((latent / 0.18215).to(vae.dtype)).sample\n",
    "                    )\n",
    "                )\n",
    "            }, step=i)\n",
    "\n",
    "        # scale and decode the image latents with vae\n",
    "        latent = latent / 0.18215\n",
    "        image = vae.decode(latent.to(vae.dtype)).sample\n",
    "\n",
    "    return postprocess(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh2De-3dl3-9"
   },
   "source": [
    "# Step 3: Enter Prompts and Additional Configs\n",
    "\n",
    "- `config.prompt`: The prompt as a string.\n",
    "- `config.prompt_edit`: The second prompt as a string, used to edit the first prompt using cross attention, set `\"\"` to disable.\n",
    "- `config.prompt_edit_token_weights`: Values to scale the importance of the tokens in cross attention layers, as a list of tuples representing `(token id, strength)`, this is used to increase or decrease the importance of a word in the prompt, it is applied to prompt_edit when possible (if `prompt_edit` is `\"\"`, weights are applied to prompt).\n",
    "- `config.prompt_edit_tokens_start`: How strict is the generation with respect to the initial prompt, increasing this will let the network be more creative for smaller details/textures, should be smaller than `prompt_edit_tokens_end`.\n",
    "- `config.prompt_edit_tokens_end`: How strict is the generation with respect to the initial prompt, decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than `prompt_edit_tokens_start`.\n",
    "- `config.prompt_edit_spatial_start`: How strict is the generation with respect to the initial image (generated from the first prompt, not from img2img), increasing this will let the network be more creative for smaller details/textures, should be smaller than `prompt_edit_spatial_end`.\n",
    "- `config.prompt_edit_spatial_end`: How strict is the generation with respect to the initial image (generated from the first prompt, not from img2img), decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than `prompt_edit_spatial_start`.\n",
    "- `config.guidance_scale`: Standard classifier-free guidance strength for stable diffusion.\n",
    "- `config.steps`: Number of diffusion steps as an integer, higher usually produces better images but is slower.\n",
    "- `config.seed`: Random Seed.\n",
    "- `config.image_width`: Width of generated image.\n",
    "- `config.image_height`: Height of generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "jGr3m8p7sqfn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> a\n",
      "2 -> photo\n",
      "3 -> of\n",
      "4 -> a\n",
      "5 -> person\n",
      "6 -> with\n",
      "7 -> butterfly\n",
      "8 -> head\n",
      "9 -> piece\n",
      "10 -> and\n",
      "11 -> elegant\n",
      "12 -> jewels\n"
     ]
    }
   ],
   "source": [
    "def display_prompt_tokens(prompt):\n",
    "    tokens = clip_tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=clip_tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_overflowing_tokens=True\n",
    "    ).input_ids[0]\n",
    "    for idx, token in enumerate(tokens):\n",
    "        decoded_token = clip_tokenizer.decode(token)\n",
    "        if decoded_token == \"<|startoftext|>\":\n",
    "            continue\n",
    "        elif decoded_token == \"<|endoftext|>\":\n",
    "            break\n",
    "        else:\n",
    "            print(idx, \"->\", decoded_token)\n",
    "\n",
    "\n",
    "# the prompt as a string\n",
    "config.prompt = \"A photo of a Person with flower headpiece and elegant jewels\" #@param {\"type\": \"string\"}\n",
    "\n",
    "# the second prompt as a string, used to edit the first prompt\n",
    "# using cross attention, set \"\" to disable\n",
    "config.prompt_edit = \"A photo of a Person with butterfly headpiece and elegant jewels\" #@param {\"type\": \"string\"}\n",
    "\n",
    "display_prompt_tokens(config.prompt_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "LgAJbmfivgjc"
   },
   "outputs": [],
   "source": [
    "# values to scale the importance of the tokens in\n",
    "# cross attention layers, as a list of tuples representing\n",
    "# (token id, strength), this is used to increase or decrease\n",
    "# the importance of a word in the prompt, it is applied to prompt_edit when possible (if prompt_edit is None, weights are applied to prompt)\n",
    "config.prompt_edit_token_weights = [(7, 4)] #@param {type:\"raw\"}\n",
    "\n",
    "# how strict is the generation with respect to the initial prompt,\n",
    "# increasing this will let the network be more creative for smaller\n",
    "# details/textures, should be smaller than prompt_edit_tokens_end\n",
    "config.prompt_edit_tokens_start = 0.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "# how strict is the generation with respect to the initial prompt,\n",
    "# decreasing this will let the network be more creative for larger\n",
    "# features/general scene composition, should be bigger than\n",
    "# prompt_edit_tokens_start\n",
    "config.prompt_edit_tokens_end = 1.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "# how strict is the generation with respect to the initial image\n",
    "# (generated from the first prompt, not from img2img), increasing\n",
    "# this will let the network be more creative for smaller\n",
    "# details/textures, should be smaller than prompt_edit_spatial_end\n",
    "config.prompt_edit_spatial_start = 0.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "# how strict is the generation with respect to the initial image\n",
    "# (generated from the first prompt, not from img2img), decreasing\n",
    "# this will let the network be more creative for larger\n",
    "# features/general scene composition, should be bigger than\n",
    "# prompt_edit_spatial_start\n",
    "config.prompt_edit_spatial_end = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "# standard classifier-free guidance strength for stable diffusion\n",
    "config.guidance_scale = 7.5 #@param {type:\"slider\", min:0, max:100, step:0.1}\n",
    "\n",
    "# number of diffusion steps as an integer, higher usually produces\n",
    "# better images but is slower\n",
    "config.steps = 50 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "\n",
    "# random seed as an integer\n",
    "config.seed = 98374234 #@param {type:\"number\"}\n",
    "\n",
    "# image width and heigh\n",
    "config.image_width = 768 #@param {type:\"slider\", min:512, max:1024, step:1}\n",
    "config.image_height = 512 #@param {type:\"slider\", min:512, max:1024, step:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mE4ZhjEn2tU"
   },
   "source": [
    "# Step 4: Generate Images with Prompt and Prompt Edits.\n",
    "\n",
    "Image genetaed will be automatically logged to the respective **Weights & Biases** workspace as an interactive [**Table**](https://docs.wandb.ai/guides/data-vis) with all configs.\n",
    "\n",
    "![](https://i.imgur.com/CqIJgPg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ltVIFaB50gf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image with Promp Edit\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m generated_image_with_prompt_edit \u001b[38;5;241m=\u001b[39m stablediffusion(\n\u001b[1;32m      4\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt,\n\u001b[1;32m      5\u001b[0m     prompt_edit\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt_edit,\n\u001b[1;32m      6\u001b[0m     prompt_edit_token_weights\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt_edit_token_weights,\n\u001b[1;32m      7\u001b[0m     prompt_edit_tokens_start\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt_edit_tokens_start,\n\u001b[1;32m      8\u001b[0m     prompt_edit_tokens_end\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt_edit_tokens_end,\n\u001b[1;32m      9\u001b[0m     prompt_edit_spatial_start\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt_edit_spatial_start,\n\u001b[1;32m     10\u001b[0m     prompt_edit_spatial_end\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt_edit_spatial_end,\n\u001b[1;32m     11\u001b[0m     guidance_scale\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mguidance_scale,\n\u001b[1;32m     12\u001b[0m     steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39msteps,\n\u001b[1;32m     13\u001b[0m     seed\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed,\n\u001b[1;32m     14\u001b[0m     width\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_width,\n\u001b[1;32m     15\u001b[0m     height\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_height,\n\u001b[1;32m     16\u001b[0m     init_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     init_image_strength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mprompt_edit \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     21\u001b[0m     generated_image_without_prompt_edit \u001b[38;5;241m=\u001b[39m stablediffusion(\n\u001b[1;32m     22\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprompt,\n\u001b[1;32m     23\u001b[0m         prompt_edit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         init_image_strength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 86\u001b[0m, in \u001b[0;36mstablediffusion\u001b[0;34m(prompt, prompt_edit, prompt_edit_token_weights, prompt_edit_tokens_start, prompt_edit_tokens_end, prompt_edit_spatial_start, prompt_edit_spatial_end, guidance_scale, steps, seed, width, height, init_image, init_image_strength)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Generate random normal noise\u001b[39;00m\n\u001b[1;32m     83\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\n\u001b[1;32m     84\u001b[0m     init_latent\u001b[38;5;241m.\u001b[39mshape, generator\u001b[38;5;241m=\u001b[39mgenerator, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     85\u001b[0m )\n\u001b[0;32m---> 86\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_noise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_latent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_start\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Process clip\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(config\u001b[38;5;241m.\u001b[39mdevice):\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.9/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:263\u001b[0m, in \u001b[0;36mLMSDiscreteScheduler.add_noise\u001b[0;34m(self, original_samples, noise, timesteps)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmas\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39moriginal_samples\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39moriginal_samples\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps\u001b[38;5;241m.\u001b[39mto(original_samples\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 263\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(original_samples\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    265\u001b[0m schedule_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timesteps, torch\u001b[38;5;241m.\u001b[39mIntTensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timesteps, torch\u001b[38;5;241m.\u001b[39mLongTensor):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "\n",
    "generated_image_with_prompt_edit = stablediffusion(\n",
    "    prompt=config.prompt,\n",
    "    prompt_edit=config.prompt_edit,\n",
    "    prompt_edit_token_weights=config.prompt_edit_token_weights,\n",
    "    prompt_edit_tokens_start=config.prompt_edit_tokens_start,\n",
    "    prompt_edit_tokens_end=config.prompt_edit_tokens_end,\n",
    "    prompt_edit_spatial_start=config.prompt_edit_spatial_start,\n",
    "    prompt_edit_spatial_end=config.prompt_edit_spatial_end,\n",
    "    guidance_scale=config.guidance_scale,\n",
    "    steps=config.steps,\n",
    "    seed=config.seed,\n",
    "    width=config.image_width,\n",
    "    height=config.image_height,\n",
    "    init_image=None,\n",
    "    init_image_strength=0.5\n",
    ")\n",
    "\n",
    "if config.prompt_edit != \"\":\n",
    "    generated_image_without_prompt_edit = stablediffusion(\n",
    "        prompt=config.prompt,\n",
    "        prompt_edit=\"\",\n",
    "        prompt_edit_token_weights=config.prompt_edit_token_weights,\n",
    "        prompt_edit_tokens_start=config.prompt_edit_tokens_start,\n",
    "        prompt_edit_tokens_end=config.prompt_edit_tokens_end,\n",
    "        prompt_edit_spatial_start=config.prompt_edit_spatial_start,\n",
    "        prompt_edit_spatial_end=config.prompt_edit_spatial_end,\n",
    "        guidance_scale=config.guidance_scale ,\n",
    "        steps=config.steps,\n",
    "        seed=config.seed,\n",
    "        width=config.image_width,\n",
    "        height=config.image_height,\n",
    "        init_image=None,\n",
    "        init_image_strength=0.5\n",
    "    )\n",
    "    table = wandb.Table(\n",
    "        columns=[\n",
    "            \"Seed\",\n",
    "            \"Guidance Scale\",\n",
    "            \"Image Height\",\n",
    "            \"Image Width\",\n",
    "            \"Number of Steps\",\n",
    "            \"Prompt\",\n",
    "            \"Image Generated With Prompt\",\n",
    "            \"Prompt Edit\",\n",
    "            \"Edit Token Weights\",\n",
    "            \"Image Generated With Prompt Edit\"\n",
    "        ]\n",
    "    )\n",
    "    table.add_data(\n",
    "        config.seed,\n",
    "        config.guidance_scale,\n",
    "        config.image_height,\n",
    "        config.image_width,\n",
    "        config.steps,\n",
    "        config.prompt,\n",
    "        wandb.Image(generated_image_without_prompt_edit),\n",
    "        config.prompt_edit,\n",
    "        config.prompt_edit_token_weights,\n",
    "        wandb.Image(generated_image_with_prompt_edit)\n",
    "    )\n",
    "    wandb.log({\n",
    "        \"Image Editing with Cross Attention Control\": table\n",
    "    })\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kr1FmAhjL76"
   },
   "source": [
    "**References:**\n",
    "- https://arxiv.org/abs/2208.01626\n",
    "- https://github.com/bloc97/CrossAttentionControl"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
